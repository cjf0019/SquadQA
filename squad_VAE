import os
import torch
import transformers
import datasets
import pytorch_lightning as pl
#from pl_bolts.callbacks import ModuleDataMonitor, TrainingDataMonitor, \
#    BatchGradientVerificationCallback, PrintTableMetricsCallback
from squad_utilities import process_generated_answer, stage_squad_data_pytorch, tokenize_to_dict, SpacyTokenizer
from squad_datasets_datamodules import NLPDataset, VAEDataModule_Sentence
from squad_models import VAE, InputProcessor, ConvolutionalEncoderDecoder, NegativeSamplingLoss

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import datetime

os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # dont use GPU for now
input_dir = "C:\\Users\\cfavr\\Documents\\Python Scripts\\"

from torch.utils.data import DataLoader
#from transformers import AdamW

RUN_NAME = 'Squad_VAE_Sentence'
BATCH_SIZE = 64
N_EPOCHS = 20
#TOK_MODEL_NAME = 't5-small'
SEED = 13
CHKPTPATH = str(datetime.date.today()) + '/checkpoints'+'_'+RUN_NAME
MODE = 'train'
LOAD_PRETRAINED_FILE = False
OUTPUT_PREDICTION_FILE = True
TRAIN_FILE = input_dir + 'squad/train-v2.0.json'
TEST_FILE = input_dir + 'squad/dev-v2.0.json'
NEG_SAMP_MODEL_NAME = 'test_embed_multivariate'

pl.seed_everything(SEED)


def run():
    tokenizer = SpacyTokenizer(sentence_separation=True)
    train_df, val_df = stage_squad_data_pytorch(TRAIN_FILE,TEST_FILE)

    with open(NEG_SAMP_MODEL_NAME+'_loc.npy', 'rb') as f:
        loc = np.load(f)

    with open(NEG_SAMP_MODEL_NAME+'_covariance.npy', 'rb') as f:
        cov = np.load(f)

    embed_dist = torch.distributions.MultivariateNormal(torch.tensor(loc), covariance_matrix=torch.tensor(cov))

    data_module = VAEDataModule_Sentence(train_df, val_df, tokenizer, batch_size=BATCH_SIZE)
    data_module.setup()

    if CHKPTPATH is not None and LOAD_PRETRAINED_FILE:
        model = SquadModel_VAE.load_from_checkpoint(CHKPTPATH + '/best-checkpoint.ckpt')
    else:
        model = SquadModel_VAE()

    checkpoint_callback = pl.callbacks.ModelCheckpoint(
        dirpath=CHKPTPATH,
        filename='best-checkpoint',
        save_top_k=3,
        verbose=True,
        monitor='val_loss',
        mode='min')

    # verification = BatchGradientVerificationCallback()
    monitor = ModuleDataMonitor(submodules=['model.lm_head'], log_every_n_steps=1000)
    # monitor = TrainingDataMonitor(log_every_n_steps=1)
    # printcallback = PrintTableMetricsCallback()
    trainer = pl.Trainer(
        checkpoint_callback=checkpoint_callback,
        max_epochs=N_EPOCHS,
        # logger=pl.loggers.TensorBoardLogger('logs/'),
        progress_bar_refresh_rate=30,
        check_val_every_n_epoch=1,
        deterministic=True, \
        callbacks=[monitor]
    )

    model.prepare_for_text_generation(data_module)

    if MODE == 'train':
        model.generate_text = False
        trainer.fit(model, data_module)

    elif MODE == 'test':
        model.freeze()
        pred = trainer.test(model, test_dataloaders=data_module.test_dataloader())

        if OUTPUT_PREDICTION_FILE:
            process_generated_answer(data_module)
